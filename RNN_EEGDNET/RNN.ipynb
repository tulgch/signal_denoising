{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Synthetic EEG+EOG Dictionary Keys (SNR Levels): dict_keys([-7, -6, -5, -4, -3, -2, -1, 0, 1, 2])\n",
      "🔹 Clean EEG Shape: (3400, 512)\n",
      "🔹 SNR Level -7: Shape = (3400, 512)\n",
      "🔹 SNR Level -6: Shape = (3400, 512)\n",
      "🔹 SNR Level -5: Shape = (3400, 512)\n",
      "🔹 SNR Level -4: Shape = (3400, 512)\n",
      "🔹 SNR Level -3: Shape = (3400, 512)\n",
      "🔹 SNR Level -2: Shape = (3400, 512)\n",
      "🔹 SNR Level -1: Shape = (3400, 512)\n",
      "🔹 SNR Level 0: Shape = (3400, 512)\n",
      "🔹 SNR Level 1: Shape = (3400, 512)\n",
      "🔹 SNR Level 2: Shape = (3400, 512)\n",
      "🔹 Clean EEG Mean: -0.16774763156951383\n",
      "🔹 Clean EEG Std Dev: 231.82429071340553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset files\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset files\n",
    "synthetic_eeg_eog_path = \"/home/tulgaa/Desktop/denoisenet/Linear_Mixing/EEG+EOG/Linear_synthetic_eeg_eog.npy\"\n",
    "clean_eeg_path = \"/home/tulgaa/Desktop/denoisenet/Linear_Mixing/EEG+EOG/EEG_all_epochs.npy\"\n",
    "\n",
    "# Load the numpy arrays\n",
    "synthetic_eeg_eog = np.load(synthetic_eeg_eog_path, allow_pickle=True).item()  # Dictionary of SNR levels\n",
    "clean_eeg = np.load(clean_eeg_path)  # Ground truth EEG\n",
    "\n",
    "# Inspect dataset keys and shapes\n",
    "print(\"🔹 Synthetic EEG+EOG Dictionary Keys (SNR Levels):\", synthetic_eeg_eog.keys())\n",
    "print(\"🔹 Clean EEG Shape:\", clean_eeg.shape)\n",
    "\n",
    "# Print shape of each SNR level data\n",
    "for snr in synthetic_eeg_eog.keys():\n",
    "    print(f\"🔹 SNR Level {snr}: Shape = {synthetic_eeg_eog[snr].shape}\")\n",
    "\n",
    "# Check statistics of clean EEG data\n",
    "print(\"🔹 Clean EEG Mean:\", np.mean(clean_eeg))\n",
    "print(\"🔹 Clean EEG Std Dev:\", np.std(clean_eeg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset shapes: Noisy EEG (34000, 512), Clean EEG (34000, 512)\n",
      "✅ Clean EEG Mean after Scaling: -0.0066, Std Dev: 0.2461\n",
      "✅ Noisy EEG Mean after Scaling: -0.0068, Std Dev: 0.4158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Expand clean EEG dataset (repeat each sample 10 times)\n",
    "X_clean = np.repeat(clean_eeg, 10, axis=0)  # Shape becomes (34000, 512)\n",
    "\n",
    "# Step 2: Stack all noisy EEG signals across SNR levels\n",
    "X_noisy = np.concatenate([synthetic_eeg_eog[snr] for snr in synthetic_eeg_eog.keys()], axis=0)  # (34000, 512)\n",
    "\n",
    "# Step 3: Normalize the data between -1 and 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "X_clean_scaled = scaler.fit_transform(X_clean)  # Normalize clean EEG\n",
    "X_noisy_scaled = scaler.transform(X_noisy)  # Normalize noisy EEG\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"✅ Final dataset shapes: Noisy EEG {X_noisy_scaled.shape}, Clean EEG {X_clean_scaled.shape}\")\n",
    "print(f\"✅ Clean EEG Mean after Scaling: {np.mean(X_clean_scaled):.4f}, Std Dev: {np.std(X_clean_scaled):.4f}\")\n",
    "print(f\"✅ Noisy EEG Mean after Scaling: {np.mean(X_noisy_scaled):.4f}, Std Dev: {np.std(X_noisy_scaled):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Train Set: Noisy EEG (27200, 512), Clean EEG (27200, 512)\n",
      "✅ Final Test Set: Noisy EEG (6800, 512), Clean EEG (6800, 512)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dictionary to store split data\n",
    "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "# Split each SNR level separately to maintain distribution\n",
    "for snr in synthetic_eeg_eog.keys():\n",
    "    noisy_snr = synthetic_eeg_eog[snr]  # Get noisy EEG for this SNR level\n",
    "    clean_snr = clean_eeg  # Corresponding clean EEG (same across all SNRs)\n",
    "    \n",
    "    # Split (80% train, 20% test)\n",
    "    X_train_snr, X_test_snr, y_train_snr, y_test_snr = train_test_split(\n",
    "        noisy_snr, clean_snr, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Store\n",
    "    X_train_list.append(X_train_snr)\n",
    "    X_test_list.append(X_test_snr)\n",
    "    y_train_list.append(y_train_snr)\n",
    "    y_test_list.append(y_test_snr)\n",
    "\n",
    "# Stack all SNR levels together\n",
    "X_train = np.vstack(X_train_list)\n",
    "X_test = np.vstack(X_test_list)\n",
    "y_train = np.vstack(y_train_list)\n",
    "y_test = np.vstack(y_test_list)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"✅ Final Train Set: Noisy EEG {X_train.shape}, Clean EEG {y_train.shape}\")\n",
    "print(f\"✅ Final Test Set: Noisy EEG {X_test.shape}, Clean EEG {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Device: cuda\n",
      "✅ LSTM Model Ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using Device: {device}\")\n",
    "\n",
    "# Define a PyTorch Dataset class\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, noisy_data, clean_data):\n",
    "        self.noisy_data = torch.tensor(noisy_data, dtype=torch.float32).unsqueeze(-1)  # (N, 512, 1)\n",
    "        self.clean_data = torch.tensor(clean_data, dtype=torch.float32).unsqueeze(-1)  # (N, 512, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.noisy_data[idx], self.clean_data[idx]\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 64\n",
    "train_dataset = EEGDataset(X_train, y_train)\n",
    "test_dataset = EEGDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the LSTM-based Denoising Model\n",
    "class LSTMDenoiser(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super(LSTMDenoiser, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMDenoiser().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for denoising\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"✅ LSTM Model Ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tulgaa/anaconda3/envs/medsam/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Epoch [1/50] | Train Loss: 49235.290175 | Test Loss: 45445.583930\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [2/50] | Train Loss: 42652.147877 | Test Loss: 40136.137650\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [3/50] | Train Loss: 37929.832449 | Test Loss: 36259.897872\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [4/50] | Train Loss: 34330.383483 | Test Loss: 33035.737332\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [5/50] | Train Loss: 31383.096916 | Test Loss: 31051.970794\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [6/50] | Train Loss: 29045.718506 | Test Loss: 28399.726489\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [7/50] | Train Loss: 26988.136144 | Test Loss: 26539.371824\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [8/50] | Train Loss: 25366.737367 | Test Loss: 25562.872518\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [9/50] | Train Loss: 24005.774113 | Test Loss: 23990.529607\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [10/50] | Train Loss: 22869.467394 | Test Loss: 22981.991010\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [11/50] | Train Loss: 21898.802112 | Test Loss: 22180.034882\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [12/50] | Train Loss: 21145.882392 | Test Loss: 21211.987323\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [13/50] | Train Loss: 20303.638938 | Test Loss: 20505.123111\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [14/50] | Train Loss: 19673.081756 | Test Loss: 20171.807708\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [15/50] | Train Loss: 19107.972187 | Test Loss: 19518.403603\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [16/50] | Train Loss: 18632.650508 | Test Loss: 19151.596570\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [17/50] | Train Loss: 18160.910476 | Test Loss: 18692.223624\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [18/50] | Train Loss: 17818.906264 | Test Loss: 18322.543708\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [19/50] | Train Loss: 17488.996000 | Test Loss: 18158.195066\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [20/50] | Train Loss: 17222.305398 | Test Loss: 17847.952787\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [21/50] | Train Loss: 17001.315731 | Test Loss: 17860.113290\n",
      "🔹 Epoch [22/50] | Train Loss: 16732.060053 | Test Loss: 17475.674723\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [23/50] | Train Loss: 16458.085938 | Test Loss: 17534.176256\n",
      "🔹 Epoch [24/50] | Train Loss: 16343.359933 | Test Loss: 17891.837964\n",
      "🔹 Epoch [25/50] | Train Loss: 16232.592675 | Test Loss: 16974.869031\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [26/50] | Train Loss: 15969.129322 | Test Loss: 16765.931020\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [27/50] | Train Loss: 15894.261413 | Test Loss: 17029.870208\n",
      "🔹 Epoch [28/50] | Train Loss: 15732.695209 | Test Loss: 16501.633926\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [29/50] | Train Loss: 15623.243364 | Test Loss: 16458.949894\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [30/50] | Train Loss: 15533.434003 | Test Loss: 16293.081730\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [31/50] | Train Loss: 15413.757436 | Test Loss: 16365.198315\n",
      "🔹 Epoch [32/50] | Train Loss: 15196.319083 | Test Loss: 16361.931604\n",
      "🔹 Epoch [33/50] | Train Loss: 15162.904547 | Test Loss: 16256.483216\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [34/50] | Train Loss: 15066.537523 | Test Loss: 16444.022780\n",
      "🔹 Epoch [35/50] | Train Loss: 15068.456625 | Test Loss: 16840.765561\n",
      "🔹 Epoch [36/50] | Train Loss: 14959.580046 | Test Loss: 16216.082451\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [37/50] | Train Loss: 14914.717702 | Test Loss: 16112.274670\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [38/50] | Train Loss: 14734.063775 | Test Loss: 16230.145681\n",
      "🔹 Epoch [39/50] | Train Loss: 14661.000443 | Test Loss: 16017.999270\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [40/50] | Train Loss: 14629.896613 | Test Loss: 16077.017313\n",
      "🔹 Epoch [41/50] | Train Loss: 14586.947999 | Test Loss: 16040.556987\n",
      "🔹 Epoch [42/50] | Train Loss: 14587.416588 | Test Loss: 16051.891200\n",
      "⏹️ Early Stopping Triggered. Training Stopped!\n",
      "✅ Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Function to train the model with Early Stopping\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, patience=3):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Learning Rate Scheduler (Reduce LR if loss plateaus)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for noisy_batch, clean_batch in train_loader:\n",
    "            noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(noisy_batch)\n",
    "            loss = criterion(outputs, clean_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for noisy_batch, clean_batch in test_loader:\n",
    "                noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n",
    "                outputs = model(noisy_batch)\n",
    "                loss = criterion(outputs, clean_batch)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_test_loss)\n",
    "\n",
    "        # Print loss for monitoring\n",
    "        print(f\"🔹 Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.6f} | Test Loss: {avg_test_loss:.6f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            early_stop_counter = 0  # Reset counter\n",
    "            torch.save(model.state_dict(), \"best_lstm_model.pth\")  # Save best model\n",
    "            print(\"✅ Model Improved! Saving...\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"⏹️ Early Stopping Triggered. Training Stopped!\")\n",
    "            break\n",
    "\n",
    "    print(\"✅ Training Completed!\")\n",
    "\n",
    "# Start Training with Early Stopping\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RRMSE-T (Time Domain): 0.533674\n",
      "✅ RRMSE-S (Spectrum Domain): 0.347567\n",
      "✅ Correlation Coefficient (CC): 0.843706\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "# Function to compute RRMSE (Relative Root Mean Square Error)\n",
    "def compute_rrmse(denoised, clean):\n",
    "    return np.sqrt(np.mean((denoised - clean) ** 2)) / np.sqrt(np.mean(clean ** 2))\n",
    "\n",
    "# Function to compute RRMSE in the Spectrum Domain (RRMSE-S)\n",
    "def compute_rrmse_spectrum(denoised, clean):\n",
    "    fft_clean = np.abs(fft(clean))  # Compute FFT\n",
    "    fft_denoised = np.abs(fft(denoised))\n",
    "    return compute_rrmse(fft_denoised, fft_clean)\n",
    "\n",
    "# Function to compute Correlation Coefficient (CC)\n",
    "def compute_cc(denoised, clean):\n",
    "    return pearsonr(denoised.flatten(), clean.flatten())[0]\n",
    "\n",
    "# Initialize metric storage\n",
    "rrmse_t_list = []\n",
    "rrmse_s_list = []\n",
    "cc_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for noisy_batch, clean_batch in test_loader:\n",
    "        noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n",
    "        denoised_batch = model(noisy_batch).cpu().numpy().squeeze()\n",
    "        clean_batch_np = clean_batch.cpu().numpy().squeeze()\n",
    "\n",
    "        # Compute metrics for each sample\n",
    "        for i in range(len(clean_batch_np)):\n",
    "            rrmse_t_list.append(compute_rrmse(denoised_batch[i], clean_batch_np[i]))\n",
    "            rrmse_s_list.append(compute_rrmse_spectrum(denoised_batch[i], clean_batch_np[i]))\n",
    "            cc_list.append(compute_cc(denoised_batch[i], clean_batch_np[i]))\n",
    "\n",
    "# Compute final averaged values\n",
    "rrmse_t = np.mean(rrmse_t_list)\n",
    "rrmse_s = np.mean(rrmse_s_list)\n",
    "cc = np.mean(cc_list)\n",
    "\n",
    "print(f\"✅ RRMSE-T (Time Domain): {rrmse_t:.6f}\")\n",
    "print(f\"✅ RRMSE-S (Spectrum Domain): {rrmse_s:.6f}\")\n",
    "print(f\"✅ Correlation Coefficient (CC): {cc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Evaluating for SNR Level: -7\n",
      "🔹 Evaluating for SNR Level: -6\n",
      "🔹 Evaluating for SNR Level: -5\n",
      "🔹 Evaluating for SNR Level: -4\n",
      "🔹 Evaluating for SNR Level: -3\n",
      "🔹 Evaluating for SNR Level: -2\n",
      "🔹 Evaluating for SNR Level: -1\n",
      "🔹 Evaluating for SNR Level: 0\n",
      "🔹 Evaluating for SNR Level: 1\n",
      "🔹 Evaluating for SNR Level: 2\n",
      "\n",
      "🔹 **Final Evaluation Per SNR Level:**\n",
      "-----------------------------------------------------\n",
      "| SNR  |  RRMSE-T  |  RRMSE-S  |   CC   |\n",
      "-----------------------------------------------------\n",
      "|  -7  |  0.604605  |  0.396976  |  0.806070  |\n",
      "|  -6  |  0.576244  |  0.379189  |  0.823137  |\n",
      "|  -5  |  0.552102  |  0.363474  |  0.837296  |\n",
      "|  -4  |  0.531786  |  0.350357  |  0.848761  |\n",
      "|  -3  |  0.514642  |  0.339950  |  0.858336  |\n",
      "|  -2  |  0.499370  |  0.330671  |  0.866802  |\n",
      "|  -1  |  0.486451  |  0.322775  |  0.873886  |\n",
      "|   0  |  0.475142  |  0.315710  |  0.880000  |\n",
      "|   1  |  0.465327  |  0.308980  |  0.885121  |\n",
      "|   2  |  0.457174  |  0.302691  |  0.889400  |\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "# Function to compute RRMSE (Relative Root Mean Square Error)\n",
    "def compute_rrmse(denoised, clean):\n",
    "    return np.sqrt(np.mean((denoised - clean) ** 2)) / np.sqrt(np.mean(clean ** 2))\n",
    "\n",
    "# Function to compute RRMSE in the Spectrum Domain (RRMSE-S)\n",
    "def compute_rrmse_spectrum(denoised, clean):\n",
    "    fft_clean = np.abs(fft(clean))  # Compute FFT\n",
    "    fft_denoised = np.abs(fft(denoised))\n",
    "    return compute_rrmse(fft_denoised, fft_clean)\n",
    "\n",
    "# Function to compute Correlation Coefficient (CC)\n",
    "def compute_cc(denoised, clean):\n",
    "    return pearsonr(denoised.flatten(), clean.flatten())[0]\n",
    "\n",
    "# Store results per SNR level\n",
    "snr_metrics = {}\n",
    "\n",
    "# Loop through each SNR level\n",
    "with torch.no_grad():\n",
    "    for snr in synthetic_eeg_eog.keys():\n",
    "        print(f\"🔹 Evaluating for SNR Level: {snr}\")\n",
    "\n",
    "        # Get test data for this SNR level\n",
    "        noisy_snr = synthetic_eeg_eog[snr][-int(0.2 * len(synthetic_eeg_eog[snr])):]  # Take 20% as test set\n",
    "        clean_snr = clean_eeg[-int(0.2 * len(clean_eeg)):]  # Corresponding clean EEG\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        noisy_tensor = torch.tensor(noisy_snr, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        clean_tensor = torch.tensor(clean_snr, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        denoised_tensor = model(noisy_tensor).cpu().numpy().squeeze()\n",
    "        clean_snr_np = clean_tensor.cpu().numpy().squeeze()\n",
    "\n",
    "        # Compute metrics\n",
    "        rrmse_t_list = [compute_rrmse(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "        rrmse_s_list = [compute_rrmse_spectrum(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "        cc_list = [compute_cc(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "\n",
    "        # Store results for this SNR level\n",
    "        snr_metrics[snr] = {\n",
    "            \"RRMSE-T\": np.mean(rrmse_t_list),\n",
    "            \"RRMSE-S\": np.mean(rrmse_s_list),\n",
    "            \"CC\": np.mean(cc_list)\n",
    "        }\n",
    "\n",
    "# Print Results in Table Format\n",
    "print(\"\\n🔹 **Final Evaluation Per SNR Level:**\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"| SNR  |  RRMSE-T  |  RRMSE-S  |   CC   |\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "for snr in sorted(snr_metrics.keys()):\n",
    "    print(f\"| {snr:3d}  |  {snr_metrics[snr]['RRMSE-T']:.6f}  |  {snr_metrics[snr]['RRMSE-S']:.6f}  |  {snr_metrics[snr]['CC']:.6f}  |\")\n",
    "print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
