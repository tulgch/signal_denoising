{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Synthetic EEG+EOG Dictionary Keys (SNR Levels): dict_keys([-7, -6, -5, -4, -3, -2, -1, 0, 1, 2])\n",
      "🔹 Clean EEG Shape: (3400, 512)\n",
      "🔹 SNR Level -7: Shape = (3400, 512)\n",
      "🔹 SNR Level -6: Shape = (3400, 512)\n",
      "🔹 SNR Level -5: Shape = (3400, 512)\n",
      "🔹 SNR Level -4: Shape = (3400, 512)\n",
      "🔹 SNR Level -3: Shape = (3400, 512)\n",
      "🔹 SNR Level -2: Shape = (3400, 512)\n",
      "🔹 SNR Level -1: Shape = (3400, 512)\n",
      "🔹 SNR Level 0: Shape = (3400, 512)\n",
      "🔹 SNR Level 1: Shape = (3400, 512)\n",
      "🔹 SNR Level 2: Shape = (3400, 512)\n",
      "🔹 Clean EEG Mean: -0.16774763156951383\n",
      "🔹 Clean EEG Std Dev: 231.82429071340553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset files\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset files\n",
    "synthetic_eeg_eog_path = \"/home/tulgaa/Desktop/denoisenet/Linear_Mixing/EEG+EOG/Linear_synthetic_eeg_eog.npy\"\n",
    "clean_eeg_path = \"/home/tulgaa/Desktop/denoisenet/Linear_Mixing/EEG+EOG/EEG_all_epochs.npy\"\n",
    "\n",
    "# Load the numpy arrays\n",
    "synthetic_eeg_eog = np.load(synthetic_eeg_eog_path, allow_pickle=True).item()  # Dictionary of SNR levels\n",
    "clean_eeg = np.load(clean_eeg_path)  # Ground truth EEG\n",
    "\n",
    "# Inspect dataset keys and shapes\n",
    "print(\"🔹 Synthetic EEG+EOG Dictionary Keys (SNR Levels):\", synthetic_eeg_eog.keys())\n",
    "print(\"🔹 Clean EEG Shape:\", clean_eeg.shape)\n",
    "\n",
    "# Print shape of each SNR level data\n",
    "for snr in synthetic_eeg_eog.keys():\n",
    "    print(f\"🔹 SNR Level {snr}: Shape = {synthetic_eeg_eog[snr].shape}\")\n",
    "\n",
    "# Check statistics of clean EEG data\n",
    "print(\"🔹 Clean EEG Mean:\", np.mean(clean_eeg))\n",
    "print(\"🔹 Clean EEG Std Dev:\", np.std(clean_eeg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset shapes: Noisy EEG (34000, 512), Clean EEG (34000, 512)\n",
      "✅ Clean EEG Mean after Scaling: -0.0066, Std Dev: 0.2461\n",
      "✅ Noisy EEG Mean after Scaling: -0.0070, Std Dev: 0.6368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Expand clean EEG dataset (repeat 10 times to match noisy EEG dataset)\n",
    "X_clean = np.repeat(clean_eeg, 10, axis=0)  # Shape becomes (34,000, 512)\n",
    "\n",
    "# Step 2: Stack all noisy EEG signals across different SNR levels\n",
    "X_noisy = np.concatenate([synthetic_eeg_eog[snr] for snr in synthetic_eeg_eog.keys()], axis=0)  # Shape (34,000, 512)\n",
    "\n",
    "# Step 3: Normalize both clean and noisy EEG signals to the range [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_clean_scaled = scaler.fit_transform(X_clean)  # Normalize clean EEG\n",
    "X_noisy_scaled = scaler.transform(X_noisy)  # Normalize noisy EEG\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"✅ Final dataset shapes: Noisy EEG {X_noisy_scaled.shape}, Clean EEG {X_clean_scaled.shape}\")\n",
    "print(f\"✅ Clean EEG Mean after Scaling: {np.mean(X_clean_scaled):.4f}, Std Dev: {np.std(X_clean_scaled):.4f}\")\n",
    "print(f\"✅ Noisy EEG Mean after Scaling: {np.mean(X_noisy_scaled):.4f}, Std Dev: {np.std(X_noisy_scaled):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Train Set: Noisy EEG (27200, 512), Clean EEG (27200, 512)\n",
      "✅ Final Test Set: Noisy EEG (6800, 512), Clean EEG (6800, 512)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize lists to store train/test sets\n",
    "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "# Split each SNR level separately to preserve SNR distribution\n",
    "for snr in synthetic_eeg_eog.keys():\n",
    "    noisy_snr = synthetic_eeg_eog[snr]  # Get noisy EEG data for this SNR level\n",
    "    clean_snr = clean_eeg  # Corresponding clean EEG data\n",
    "\n",
    "    # Split into 80% Train, 20% Test\n",
    "    X_train_snr, X_test_snr, y_train_snr, y_test_snr = train_test_split(\n",
    "        noisy_snr, clean_snr, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Store the results\n",
    "    X_train_list.append(X_train_snr)\n",
    "    X_test_list.append(X_test_snr)\n",
    "    y_train_list.append(y_train_snr)\n",
    "    y_test_list.append(y_test_snr)\n",
    "\n",
    "# Stack all train and test data together\n",
    "X_train = np.vstack(X_train_list)\n",
    "X_test = np.vstack(X_test_list)\n",
    "y_train = np.vstack(y_train_list)\n",
    "y_test = np.vstack(y_test_list)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"✅ Final Train Set: Noisy EEG {X_train.shape}, Clean EEG {y_train.shape}\")\n",
    "print(f\"✅ Final Test Set: Noisy EEG {X_test.shape}, Clean EEG {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model is initialized on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the EEGDiR LSTM Model\n",
    "class EEGDiR_LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super(EEGDiR_LSTM, self).__init__()\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)  \n",
    "        self.fc2 = nn.Linear(64, 32)  \n",
    "        self.fc3 = nn.Linear(32, input_size)  \n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  \n",
    "        x = self.relu(self.fc1(lstm_out))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output the denoised EEG\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EEGDiR_LSTM().to(device)\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"✅ Model is initialized on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders are ready! Training Batch Size: 1024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom PyTorch Dataset\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, noisy_data, clean_data):\n",
    "        self.noisy_data = torch.tensor(noisy_data, dtype=torch.float32).unsqueeze(-1)  # (N, 512, 1)\n",
    "        self.clean_data = torch.tensor(clean_data, dtype=torch.float32).unsqueeze(-1)  # (N, 512, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.noisy_data[idx], self.clean_data[idx]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1024\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = EEGDataset(X_train, y_train)\n",
    "test_dataset = EEGDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✅ DataLoaders are ready! Training Batch Size: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tulgaa/anaconda3/envs/medsam/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Epoch [1/50] | Train Loss: 51942.156105 | Test Loss: 47763.353237\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [2/50] | Train Loss: 39783.776910 | Test Loss: 30532.616908\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [3/50] | Train Loss: 25306.262297 | Test Loss: 20927.696708\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [4/50] | Train Loss: 20844.667535 | Test Loss: 18609.704241\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [5/50] | Train Loss: 19159.754630 | Test Loss: 17776.962472\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [6/50] | Train Loss: 18051.425130 | Test Loss: 17040.471959\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [7/50] | Train Loss: 17171.120515 | Test Loss: 16524.702846\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [8/50] | Train Loss: 16748.587059 | Test Loss: 16428.017160\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [9/50] | Train Loss: 16245.704246 | Test Loss: 16235.419224\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [10/50] | Train Loss: 15924.886936 | Test Loss: 15902.839704\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [11/50] | Train Loss: 15439.057979 | Test Loss: 15676.043806\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [12/50] | Train Loss: 15343.089048 | Test Loss: 15762.118304\n",
      "🔹 Epoch [13/50] | Train Loss: 15003.973669 | Test Loss: 15373.542690\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [14/50] | Train Loss: 14833.530273 | Test Loss: 15455.387416\n",
      "🔹 Epoch [15/50] | Train Loss: 14381.351345 | Test Loss: 15272.966099\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [16/50] | Train Loss: 14201.156973 | Test Loss: 15378.190709\n",
      "🔹 Epoch [17/50] | Train Loss: 13867.319083 | Test Loss: 15262.665876\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [18/50] | Train Loss: 13653.509874 | Test Loss: 15218.208705\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [19/50] | Train Loss: 13342.493273 | Test Loss: 15278.882533\n",
      "🔹 Epoch [20/50] | Train Loss: 13104.112449 | Test Loss: 15236.070871\n",
      "🔹 Epoch [21/50] | Train Loss: 12779.529586 | Test Loss: 15181.675921\n",
      "✅ Model Improved! Saving...\n",
      "🔹 Epoch [22/50] | Train Loss: 12752.022135 | Test Loss: 15779.053711\n",
      "🔹 Epoch [23/50] | Train Loss: 12550.511683 | Test Loss: 15878.341239\n",
      "🔹 Epoch [24/50] | Train Loss: 12192.206091 | Test Loss: 15346.214704\n",
      "⏹️ Early Stopping Triggered. Training Stopped!\n",
      "✅ Training Completed!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Function to train the model with Early Stopping\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, patience=3):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Learning Rate Scheduler (Reduce LR if loss plateaus)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for noisy_batch, clean_batch in train_loader:\n",
    "            noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(noisy_batch)\n",
    "            loss = criterion(outputs, clean_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for noisy_batch, clean_batch in test_loader:\n",
    "                noisy_batch, clean_batch = noisy_batch.to(device), clean_batch.to(device)\n",
    "                outputs = model(noisy_batch)\n",
    "                loss = criterion(outputs, clean_batch)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_test_loss)\n",
    "\n",
    "        # Print loss for monitoring\n",
    "        print(f\"🔹 Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.6f} | Test Loss: {avg_test_loss:.6f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            early_stop_counter = 0  # Reset counter\n",
    "            torch.save(model.state_dict(), \"best_lstm_model.pth\")  # Save best model\n",
    "            print(\"✅ Model Improved! Saving...\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"⏹️ Early Stopping Triggered. Training Stopped!\")\n",
    "            break\n",
    "\n",
    "    print(\"✅ Training Completed!\")\n",
    "\n",
    "# Start Training with Early Stopping\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Evaluating for SNR Level: -7\n",
      "🔹 Evaluating for SNR Level: -6\n",
      "🔹 Evaluating for SNR Level: -5\n",
      "🔹 Evaluating for SNR Level: -4\n",
      "🔹 Evaluating for SNR Level: -3\n",
      "🔹 Evaluating for SNR Level: -2\n",
      "🔹 Evaluating for SNR Level: -1\n",
      "🔹 Evaluating for SNR Level: 0\n",
      "🔹 Evaluating for SNR Level: 1\n",
      "🔹 Evaluating for SNR Level: 2\n",
      "\n",
      "🔹 **Final Evaluation Per SNR Level:**\n",
      "-----------------------------------------------------\n",
      "| SNR  |  RRMSE-T  |  RRMSE-S  |   CC   |\n",
      "-----------------------------------------------------\n",
      "|  -7  |  0.925227  |  0.633353  |  0.612503  |\n",
      "|  -6  |  0.797495  |  0.533744  |  0.685796  |\n",
      "|  -5  |  0.689837  |  0.457695  |  0.750957  |\n",
      "|  -4  |  0.604933  |  0.403349  |  0.803887  |\n",
      "|  -3  |  0.539153  |  0.363159  |  0.843665  |\n",
      "|  -2  |  0.493292  |  0.336257  |  0.870625  |\n",
      "|  -1  |  0.459594  |  0.316222  |  0.888859  |\n",
      "|   0  |  0.435934  |  0.302739  |  0.901197  |\n",
      "|   1  |  0.422188  |  0.293768  |  0.908183  |\n",
      "|   2  |  0.415177  |  0.289460  |  0.912095  |\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "# Load the best trained model\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Function to compute RRMSE (Relative Root Mean Square Error)\n",
    "def compute_rrmse(denoised, clean):\n",
    "    return np.sqrt(np.mean((denoised - clean) ** 2)) / np.sqrt(np.mean(clean ** 2))\n",
    "\n",
    "# Function to compute RRMSE in the Spectrum Domain (RRMSE-S)\n",
    "def compute_rrmse_spectrum(denoised, clean):\n",
    "    fft_clean = np.abs(fft(clean))  # Compute FFT\n",
    "    fft_denoised = np.abs(fft(denoised))\n",
    "    return compute_rrmse(fft_denoised, fft_clean)\n",
    "\n",
    "# Function to compute Correlation Coefficient (CC)\n",
    "def compute_cc(denoised, clean):\n",
    "    return pearsonr(denoised.flatten(), clean.flatten())[0]\n",
    "\n",
    "# Store results per SNR level\n",
    "snr_metrics = {}\n",
    "\n",
    "# Loop through each SNR level\n",
    "with torch.no_grad():\n",
    "    for snr in synthetic_eeg_eog.keys():\n",
    "        print(f\"🔹 Evaluating for SNR Level: {snr}\")\n",
    "\n",
    "        # Get test data for this SNR level\n",
    "        noisy_snr = synthetic_eeg_eog[snr][-int(0.2 * len(synthetic_eeg_eog[snr])):]  # Take 20% as test set\n",
    "        clean_snr = clean_eeg[-int(0.2 * len(clean_eeg)):]  # Corresponding clean EEG\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        noisy_tensor = torch.tensor(noisy_snr, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        clean_tensor = torch.tensor(clean_snr, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        denoised_tensor = model(noisy_tensor).cpu().numpy().squeeze()\n",
    "        clean_snr_np = clean_tensor.cpu().numpy().squeeze()\n",
    "\n",
    "        # Compute metrics\n",
    "        rrmse_t_list = [compute_rrmse(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "        rrmse_s_list = [compute_rrmse_spectrum(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "        cc_list = [compute_cc(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "\n",
    "        # Store results for this SNR level\n",
    "        snr_metrics[snr] = {\n",
    "            \"RRMSE-T\": np.mean(rrmse_t_list),\n",
    "            \"RRMSE-S\": np.mean(rrmse_s_list),\n",
    "            \"CC\": np.mean(cc_list)\n",
    "        }\n",
    "\n",
    "# Print Results in Table Format\n",
    "print(\"\\n🔹 **Final Evaluation Per SNR Level:**\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"| SNR  |  RRMSE-T  |  RRMSE-S  |   CC   |\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "for snr in sorted(snr_metrics.keys()):\n",
    "    print(f\"| {snr:3d}  |  {snr_metrics[snr]['RRMSE-T']:.6f}  |  {snr_metrics[snr]['RRMSE-S']:.6f}  |  {snr_metrics[snr]['CC']:.6f}  |\")\n",
    "print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Evaluating for SNR Level: -7\n",
      "🔹 Evaluating for SNR Level: -6\n",
      "🔹 Evaluating for SNR Level: -5\n",
      "🔹 Evaluating for SNR Level: -4\n",
      "🔹 Evaluating for SNR Level: -3\n",
      "🔹 Evaluating for SNR Level: -2\n",
      "🔹 Evaluating for SNR Level: -1\n",
      "🔹 Evaluating for SNR Level: 0\n",
      "🔹 Evaluating for SNR Level: 1\n",
      "🔹 Evaluating for SNR Level: 2\n",
      "\n",
      "🔹 **Final Evaluation Per SNR Level:**\n",
      "-----------------------------------------------------\n",
      "| SNR  |  RRMSE-T  |  RRMSE-S  |   CC   |\n",
      "-----------------------------------------------------\n",
      "|  -7  |  0.925227  |  0.633353  |  0.612503  |\n",
      "|  -6  |  0.797495  |  0.533744  |  0.685796  |\n",
      "|  -5  |  0.689837  |  0.457695  |  0.750957  |\n",
      "|  -4  |  0.604933  |  0.403349  |  0.803887  |\n",
      "|  -3  |  0.539153  |  0.363159  |  0.843665  |\n",
      "|  -2  |  0.493292  |  0.336257  |  0.870625  |\n",
      "|  -1  |  0.459594  |  0.316222  |  0.888859  |\n",
      "|   0  |  0.435934  |  0.302739  |  0.901197  |\n",
      "|   1  |  0.422188  |  0.293768  |  0.908183  |\n",
      "|   2  |  0.415177  |  0.289460  |  0.912095  |\n",
      "-----------------------------------------------------\n",
      "🔹 **Overall Averages**: RRMSE-T: 0.578283, RRMSE-S: 0.392974, CC: 0.817777\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "# Load the best trained model\n",
    "model.load_state_dict(torch.load(\"best_lstm_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Function to compute RRMSE (Relative Root Mean Square Error)\n",
    "def compute_rrmse(denoised, clean):\n",
    "    return np.sqrt(np.mean((denoised - clean) ** 2)) / np.sqrt(np.mean(clean ** 2))\n",
    "\n",
    "# Function to compute RRMSE in the Spectrum Domain (RRMSE-S)\n",
    "def compute_rrmse_spectrum(denoised, clean):\n",
    "    fft_clean = np.abs(fft(clean))  # Compute FFT\n",
    "    fft_denoised = np.abs(fft(denoised))\n",
    "    return compute_rrmse(fft_denoised, fft_clean)\n",
    "\n",
    "# Function to compute Correlation Coefficient (CC)\n",
    "def compute_cc(denoised, clean):\n",
    "    return pearsonr(denoised.flatten(), clean.flatten())[0]\n",
    "\n",
    "# Store results per SNR level\n",
    "snr_metrics = {}\n",
    "all_rrmse_t = []\n",
    "all_rrmse_s = []\n",
    "all_cc = []\n",
    "\n",
    "# Loop through each SNR level\n",
    "with torch.no_grad():\n",
    "    for snr in synthetic_eeg_eog.keys():\n",
    "        print(f\"🔹 Evaluating for SNR Level: {snr}\")\n",
    "\n",
    "        # Get test data for this SNR level\n",
    "        noisy_snr = synthetic_eeg_eog[snr][-int(0.2 * len(synthetic_eeg_eog[snr])):]  # Take 20% as test set\n",
    "        clean_snr = clean_eeg[-int(0.2 * len(clean_eeg)):]  # Corresponding clean EEG\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        noisy_tensor = torch.tensor(noisy_snr, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        clean_tensor = torch.tensor(clean_snr, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        denoised_tensor = model(noisy_tensor).cpu().numpy().squeeze()\n",
    "        clean_snr_np = clean_tensor.cpu().numpy().squeeze()\n",
    "\n",
    "        # Compute metrics\n",
    "        rrmse_t_list = [compute_rrmse(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "        rrmse_s_list = [compute_rrmse_spectrum(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "        cc_list = [compute_cc(denoised_tensor[i], clean_snr_np[i]) for i in range(len(clean_snr_np))]\n",
    "\n",
    "        # Store results for this SNR level\n",
    "        snr_metrics[snr] = {\n",
    "            \"RRMSE-T\": np.mean(rrmse_t_list),\n",
    "            \"RRMSE-S\": np.mean(rrmse_s_list),\n",
    "            \"CC\": np.mean(cc_list)\n",
    "        }\n",
    "\n",
    "        # Collect all values for averaging\n",
    "        all_rrmse_t.extend(rrmse_t_list)\n",
    "        all_rrmse_s.extend(rrmse_s_list)\n",
    "        all_cc.extend(cc_list)\n",
    "\n",
    "# Compute overall averages\n",
    "avg_rrmse_t = np.mean(all_rrmse_t)\n",
    "avg_rrmse_s = np.mean(all_rrmse_s)\n",
    "avg_cc = np.mean(all_cc)\n",
    "\n",
    "# Print Results in Table Format\n",
    "print(\"\\n🔹 **Final Evaluation Per SNR Level:**\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"| SNR  |  RRMSE-T  |  RRMSE-S  |   CC   |\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "for snr in sorted(snr_metrics.keys()):\n",
    "    print(f\"| {snr:3d}  |  {snr_metrics[snr]['RRMSE-T']:.6f}  |  {snr_metrics[snr]['RRMSE-S']:.6f}  |  {snr_metrics[snr]['CC']:.6f}  |\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(f\"🔹 **Overall Averages**: RRMSE-T: {avg_rrmse_t:.6f}, RRMSE-S: {avg_rrmse_s:.6f}, CC: {avg_cc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
